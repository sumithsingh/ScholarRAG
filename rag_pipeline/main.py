# rag_pipeline/main.py

import os
import requests
import time
from typing import List, Dict, Any

from langchain_community.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
# MODIFIED: We import the building blocks for a more robust, custom chain
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.prompts import PromptTemplate

from config import (
    EMBEDDING_MODEL_NAME, LLM_TEMPERATURE, VECTOR_DB_SEARCH_NEIGHBORS,
    CHUNK_SIZE, CHUNK_OVERLAP, SEMANTIC_SCHOLAR_API_URL,
    MAX_SEARCH_RESULTS, SEARCH_RETRY_ATTEMPTS
)

def search_semantic_scholar(query: str, api_key: str) -> List[Dict[str, Any]]:
    """Searches Semantic Scholar for academic papers with retry logic."""
    params = {"query": query, "limit": MAX_SEARCH_RESULTS, "fields": "title,abstract,url"}
    headers = {"x-api-key": api_key}
    
    for attempt in range(SEARCH_RETRY_ATTEMPTS):
        try:
            response = requests.get(SEMANTIC_SCHOLAR_API_URL, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json().get("data", [])
            return [{"title": p.get("title", "No title"), "abstract": p.get("abstract") or "No abstract available.", "url": p.get("url", "No URL")} for p in data]
        except Exception as e:
            print(f"Error fetching from Semantic Scholar on attempt {attempt + 1}: {e}")
            time.sleep(2)
    return []

def build_vector_store(papers: List[Dict[str, Any]]) -> Chroma:
    """Builds a Chroma vector store from paper abstracts."""
    docs = []
    splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    
    for paper in papers:
        if paper['abstract'] and paper['abstract'] != "No abstract available.":
            chunks = splitter.split_text(paper['abstract'])
            for chunk in chunks:
                docs.append(Document(page_content=chunk, metadata={"source": paper['url']}))

    if not docs:
        raise ValueError("No valid abstracts found in the search results to build a knowledge base.")

    embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    db = Chroma.from_documents(docs, embedding=embedder)
    return db

def process_query(query: str, api_key: str) -> Dict[str, str]:
    """
    The main function to process a user query using a custom-prompt RAG pipeline.
    """
    if not os.getenv("GOOGLE_API_KEY"):
        return {"answer": "Error: GOOGLE_API_KEY not found in environment variables.", "sources": ""}

    papers = search_semantic_scholar(query, api_key)
    if not papers:
        return {"answer": "I could not find any relevant academic papers for your query. Please try a different query.", "sources": ""}
    
    try:
        # 1. Build the knowledge base and retrieve relevant documents
        vector_db = build_vector_store(papers)
        retriever = vector_db.as_retriever(search_kwargs={"k": VECTOR_DB_SEARCH_NEIGHBORS})
        retrieved_docs = retriever.invoke(query)

        if not retrieved_docs:
             return {"answer": "Could not find a specific answer in the retrieved papers. Please try rephrasing your query.", "sources": ""}

        # 2. Engineer a clear and explicit prompt for the LLM
        prompt_template = """
        You are a helpful research assistant. Answer the user's question based ONLY on the following sources.
        Do not use any other information. If the answer is not found in the sources, say "I could not find a definitive answer in the provided sources."

        SOURCES:
        {context}

        QUESTION:
        {question}

        YOUR ANSWER:
        """
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        # 3. Define the LLM and the custom chain
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=LLM_TEMPERATURE)
        llm_chain = LLMChain(llm=llm, prompt=prompt)
        
        # This chain takes our documents, stuffs them into the 'context' variable of the prompt, and runs the LLMChain
        stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="context")
        
        # 4. Run the chain and get the response
        response = stuff_chain.invoke({"input_documents": retrieved_docs, "question": query})
        
        # 5. Manually collect the sources to guarantee they are correct
        sources = set(doc.metadata['source'] for doc in retrieved_docs)
        
        return {
            "answer": response.get('output_text', 'No answer was generated by the model.'),
            "sources": "\n".join(sources)
        }
        
    except Exception as e:
        print(f"An error occurred during the RAG process: {e}")
        return {"answer": f"An error occurred while processing your request. Details: {e}", "sources": ""}